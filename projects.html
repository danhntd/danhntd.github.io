<!DOCTYPE HTML>
<!--
	DANH NGUYEN THANH Profile
	Future Imperfect by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Thanh-Danh Nguyen - Homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" type="image/ico" href="/images/logo/favicon.ico" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Danh's Portfolio</a></h1>
						<nav class="links">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="cv.html">Curriculum Vitae</a></li>
								<li><a href="research.html">Research</a></li>
								<li><a href="projects.html"><b>Projects</b></a></li>
								<!-- <li><a href="materials.html">Materials</a></li> -->
								<li><a href="contact.html">Contact</a></li>
								


							</ul>
						</nav>
						<nav class="main">
							<ul>
								<li class="menu">
									<a class="fa-bars" href="#menu">Menu</a>
								</li>
							</ul>
						</nav>
					</header>

				<!-- Menu -->
					<section id="menu">

						<!-- Links -->
							<section>
								<ul class="links">
									<li><a href="index.html">Home</a></li>
									<li><a href="cv.html">Curriculum Vitae</a></li>
									<li><a href="research.html">Research</a></li>
									<li><a href="projects.html"><b>Projects</b></a></li>
									<!-- <li><a href="materials.html">Materials</a></li> -->
									<li><a href="contact.html">Contact</a></li>
								</ul>
							</section>

					</section>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
						<article class="post" id="CAMO-GenOS">
							<header>
								<div class="title">
									<h2><a href="#">CAMO-GenOS</a></h2>
									<p>Generative One-Shot Camouflage Instance Segmentation</p>
								</div>
								<div class="meta">
									<time class="published">Jul 2025</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\camo_genos_project.jpg" alt=""  /></a>

								<p align="justify">Identifying camouflaged instances is a critical yet underexplored problem in computer 
									vision, where traditional segmentation models often fail due to extreme visual similarity between 
									foreground and background. While recent advances have shown promise with deep learning models, they 
									heavily depend on large annotated datasets, which are costly and impractical to collect in camouflage 
									scenarios. In this work, we tackle this limitation by introducing a novel framework, dubbed CAMO GenOS, 
									that leverages one-shot annotated samples to drive a generative process for data enrichment. Our 
									approach integrates prompt-guided and mask-conditioned generative mechanisms to synthesize diverse, 
									high-fidelity camouflaged instances, thereby enhancing the learning capacity of segmentation models 
									under minimal supervision. We demonstrate the effectiveness of our CAMO-GenOS by setting up a novel 
									state-of-the-art baseline for one-shot camouflage instance segmentation research on the challenging 
									CAMO-FS benchmark.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/danhntd/CAMO-GenOS" class="button medium">Code</a>
									<a style="color:rgb(0, 85, 255);" href="pdf\MAPR2025_CAMOGenOS_Poster.pdf" class="button medium">Poster</a>
									<a style="color:rgb(0, 85, 255);" href="pdf\MAPR2025_CAMOGenOS_Slides.pdf" class="button medium">Presentation</a>
								</p>

								<b>Reference</b>:
								<blockquote>
									<b>Thanh-Danh Nguyen</b>,  Vinh-Tiep Nguyen†, and Tam V. Nguyen 
										“Generative One-shot Camouflage Instance Segmentation.” 
										2025 International Conference on Multimedia Analysis and Pattern Recognition (MAPR). IEEE, 2025. (Scopus) 
										[<a style="color:rgb(0, 85, 255);" href="https://ieeexplore.ieee.org/">DOI</a>]
								</blockquote>
						</article>

						<!-- Post -->
						<article class="post" id="FS-CAMOFreq">
							<header>
								<div class="title">
									<h2><a href="#">FS-CAMOFreq</a></h2>
									<p>Few-Shot Instance Segmentation: An Exploration in the Frequency Domain for Camouflage Instances</p>
								</div>
								<div class="meta">
									<time class="published">Jul 2025</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\fs_camofreq_project.jpg" alt=""  /></a>

								<p align="justify">Few-shot instance segmentation is an intense yet essential task, particularly in camouflaged 
									scenarios where visual ambiguity between foreground and background makes instance level recognition more 
									difficult. Prior approaches primarily focused on image augmentations in the color space domain to provide 
									diverse perspective information to the segmentation models. However, this type of augmentation often fails 
									to capture the full range of visual characteristics needed for robust generalization, particularly in 
									camouflage images, due to the limited similar representation in the color space domain. To this end, we 
									tackle this gap by exploiting a novel approach to augment and enhance image features in the derivative 
									frequency domain. Accordingly, we propose a novel framework tailored for few-shot camouflage instance 
									segmentation via the instance-aware frequency-based augmentation, dubbed FS-CAMOFreq, to enhance image 
									diversity while preserving semantic structure, thereby improving the ability of the few-shot segmentor 
									to learn from limited data. Extensive experiments on the challenging CAMO-FS benchmark demonstrate that 
									our approach achieves superior performance compared to state-of-the-art baselines.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/danhntd/FS-CAMOFreq" class="button medium">Code</a>
									<a style="color:rgb(0, 85, 255);" href="pdf\MAPR2025_FSCAMOFreq_Poster.pdf" class="button medium">Poster</a>
									<a style="color:rgb(0, 85, 255);" href="pdf\MAPR2025_FSCAMOFreq_Slides.pdf" class="button medium">Presentation</a>
								</p>

								<b>Reference</b>:
								<blockquote>
									<b>Thanh-Danh Nguyen</b>, Hung-Phu Cao, Thanh Duc Ngo, Vinh-Tiep Nguyen†, and Tam V. Nguyen, 
										“Few-Shot Instance Segmentation: An Exploration in the Frequency Domain for Camouflage Instances.” 
										2025 International Conference on Multimedia Analysis and Pattern Recognition (MAPR). IEEE, 2025. (Scopus) 
										[<a style="color:rgb(0, 85, 255);" href="https://ieeexplore.ieee.org/">DOI</a>]
								</blockquote>
						</article>

						<!-- Post -->
						<article class="post" id="CAMUL">
							<header>
								<div class="title">
									<h2><a href="#">CAMUL</a></h2>
									<p>Context-Aware Multi-conditional Instance Synthesis for Image Segmentation</p>
								</div>
								<div class="meta">
									<time class="published">Jun 2025</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\camul_project.jpg" alt=""  /></a>

								<p align="justify">Instance image segmentation task requires training with abundant annotated data to achieve 
									high accuracy. Recently, conditional image synthesis has demonstrated its effectiveness in generating 
									synthetic data for this task. However, existing image synthesis models face challenges in generating target 
									instances to match the masks with complex shapes. Moreover, others fail to create diverse instances due to 
									utilizing low-context simple text prompts. To address these issues, we propose CAMUL, a framework for 
									context-aware multi-conditional instance synthesis. CAMUL introduces two key innovations: CARP 
									(cross-attention refinement prompting) to enhance the alignment of generated instances with conditional 
									masks, and iCAFF (incremental context-aware feature fusion) to determine the general embeddings of the 
									instances for a more precise context understanding. Our method significantly improves segmentation 
									performance, increasing up to 15.34% AP on Cityscapes and 3.34% AP on the large-scale ADE20K benchmark 
									compared to the baselines.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/danhntd/CAMUL" class="button medium">Code</a>
								</p>
							
								<b>Reference</b>:
								<blockquote>
									<b>Thanh-Danh Nguyen</b>, Trong-Tai Dam Vu, Bich-Nga Pham, Thanh Duc Ngo, Tam V. Nguyen, and Vinh-Tiep Nguyen†,
									“CAMUL: Context-Aware Multi-conditional Instance Synthesis for Image Segmentation”, 
									IEEE MultiMedia, Jun 2025. IF = 3.3 (SCIE) 
									[<a style="color:rgb(0, 85, 255);" href="https://doi.org/10.1109/MMUL.2025.3578972">DOI</a>]
								</blockquote>
						</article>

						<!-- Post -->
						<article class="post" id="LTSP">
							<header>
								<div class="title">
									<h2><a href="#">LTSP</a></h2>
									<p>Nighttime Scene Understanding with Label Transfer Scene Parser</p>
								</div>
								<div class="meta">
									<time class="published">Sep 2024</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\ltsp_project.jpg" alt=""  /></a>

								<p align="justify">Semantic segmentation plays a crucial role in traffic scene understanding, especially in nighttime condition. 
								This paper tackles the task of semantic segmentation on nighttime scenes. The largest challenge of this task is the lack of 
								annotated nighttime images to train a deep learning-based scene parser. The existing annotated datasets are abundant in daytime 
								condition but scarce in nighttime due to the high cost. Thus, we propose a novel Label Transfer Scene Parser (LTSP) framework 
								for nighttime scene semantic segmentation by leveraging daytime annotation transfer. Our framework performs segmentation in 
								the dark without training on real nighttime annotated data. In particular, we propose translating daytime images to 
								nighttime condition to obtain more data with annotation in an efficient way. In addition, we utilize the pseudo-labels 
								inferred from unlabeled nighttime scenes to further train the scene parser. The novelty of our work is the ability to 
								perform nighttime segmentation via daytime annotated label and nighttime synthetic versions of the same set of images. 
								The extensive experiments demonstrate the improvement and efficiency of our scene parser over the state-of-the-art methods 
								with the similar semi-supervised approach on the benchmark of Nighttime Driving Test dataset. Notably, our proposed method 
								utilizes only one tenth of the amount of labeled and unlabeled data in comparison with the previous methods.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/danhntd/Label_Transfer_Scene_Parser" class="button medium">Code</a>
								</p>

								<b>Reference</b>:
								<blockquote>
									<b>Thanh-Danh Nguyen</b>, Nguyen Phan, Tam V. Nguyen†, Vinh-Tiep Nguyen, and Minh-Triet Tran, 
										“Nighttime Scene Understanding with Label Transfer Scene Parser”,
                                        Image and Vision Computing, Sep 2024.
										[<a style="color:rgb(0, 85, 255);" href="https://doi.org/10.1016/j.imavis.2024.105257">DOI</a>]
								</blockquote>
						</article>

						
						
						<!-- Post -->
						<article class="post" id="CAMO-FS">
							<header>
								<div class="title">
									<h2><a href="#">CAMO-FS</a></h2>
									<p>Few-shot Learning for Animal Detection and Segmentation</p>
								</div>
								<div class="meta">
									<time class="published">Jul 2024</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\camofs_project.jpg" alt=""  /></a>

								<p align="justify">Camouflaged object detection and segmentation is a new and challenging research topic in computer vision. 
									There is a serious issue of lacking data on concealed objects such as camouflaged animals in natural scenes. In this paper, 
									we address the problem of few-shot learning for camouflaged object detection and segmentation. To this end, we first collect 
									a new dataset, CAMO-FS, for the benchmark. As camouflaged instances are challenging to recognize due to their similarity compared 
									to the surroundings, we guide our models to obtain camouflaged features that highly distinguish the instances from the background. 
									In this work, we propose FS-CDIS, a framework to efficiently detect and segment camouflaged instances via two loss functions 
									contributing to the training process. Firstly, the instance triplet loss with the characteristic of differentiating the anchor, 
									which is the mean of all camouflaged foreground points, and the background points are employed to work at the instance level. 
									Secondly, to consolidate the generalization at the class level, we present instance memory storage with the scope of storing 
									camouflaged features of the same category, allowing the model to capture further class-level information during the learning 
									process. The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly 
									collected dataset.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/danhntd/FS-CDIS" class="button medium">Code</a>
									<a style="color:rgb(0, 85, 255);" href="https://www.kaggle.com/datasets/danhnt/camo-fs-dataset" class="button medium">Dataset</a>
									<a style="color:rgb(0, 85, 255);" href="pdf\IEEEAccess_CamoFS_Poster.pdf" class="button medium">Poster</a>
								</p>
									
								<b>Reference</b>:
								<blockquote>
									<b>Thanh-Danh Nguyen</b>*, Anh-Khoa Nguyen Vu*, Nhat-Duy Nguyen*, Vinh-Tiep Nguyen, Thanh Duc Ngo, Thanh-Toan Do, 
										Minh-Triet Tran, and Tam V. Nguyen†, “The Art of Camouflage: Few-shot Learning for Animal Detection and Segmentation”,
                                        IEEE Access, Jul 2024. IF = 3.4 (SCIE) 
										[<a style="color:rgb(0, 85, 255);" href="https://ieeexplore.ieee.org/document/10608133">DOI</a>,
										<a style="color:rgb(0, 85, 255);" href="https://arxiv.org/abs/2304.07444">ArXiv</a>]
								</blockquote>
						</article>
						
						<!-- Post -->
						<article class="post" id="InstSynth">
							<header>
								<div class="title">
									<h2><a href="#">InstSynth</a></h2>
									<p>Conditional Data Synthesis for Scene Understanding</p>
								</div>
								<div class="meta">
									<time class="published">Jun 2024</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\instsynth_project.jpg" alt=""  /></a>

								<p align="justify">Scene understanding at the instance level is an essential task in computer vision to support modern 
								Advanced Driver Assistance Systems. Solutions have been proposed with abundant annotated training data. However, the annotation at 
								the instance level is high-cost due to huge manual efforts. In this work, we solve this problem by introducing 
								InstSynth, an advanced framework leveraging instance-wise annotations as conditions to enrich the training data. 
								Existing methods focused on semantic segmentation via using prompts to synthesize image-annotation pairs, facing 
								an unrealistic manner. Our proposals utilize the strength of such large generative models to synthesize instance 
								data with prompt-guided and maskbased mechanisms to boost the performance of the instancelevel scene understanding 
								models. We empirically improve the performance of the latest instance segmentation architectures of FastInst and 
								OneFormer by 14.49% and 11.59% AP, respectively, evaluated on the Cityscapes benchmark. Accordingly, we construct 
								an instance-level synthesized dataset, dubbed IS-Cityscapes, with over a 4× larger number of instances in comparison 
								with the vanilla Cityscapes.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/danhntd/InstSynth" class="button medium">Code</a>
									<a style="color:rgb(0, 85, 255);" href="pdf\MAPR2024_InstSynth_Poster.pdf" class="button medium">Poster</a>
									<a style="color:rgb(0, 85, 255);" href="pdf\MAPR2024_InstSynth_Slides.pdf" class="button medium">Presentation</a>
								</p>
							
								<b>Reference</b>:
								<blockquote>
									<b>Thanh-Danh Nguyen</b>, Bich-Nga Pham, Trong-Tai Dam Vu, Vinh-Tiep Nguyen†, Thanh Duc Ngo, and Tam V. Nguyen. 
									“InstSynth: Instance-wise Prompt-guided Style Masked Conditional Data Synthesis for Scene Understanding.” 
									2024 International Conference on Multimedia Analysis and Pattern Recognition (MAPR). IEEE, 2024. (Scopus) 
									[<a style="color:rgb(0, 85, 255);" href="https://doi.org/10.1109/MAPR63514.2024.10660775">DOI</a>]
								</blockquote>
						</article>
						
						<!-- Post -->
						<article class="post" id="CE-OST">
							<header>
								<div class="title">
									<h2><a href="#">CE-OST</a></h2>
									<p>Contour Emphasis for Camouflage Instance Segmentation</p>
								</div>
								<div class="meta">
									<time class="published">Oct 2023</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\ceost_project.jpg" alt=""  /></a>

								<p align="justify">Understanding camouflage images at instance level is such a challenging task in computer vision. 
								Since the camouflage instances have their colors and textures similar to the background, the key to distinguish 
								them in the images should rely on their contours. The contours seperate the instance from the background, thus 
								recognizing these contours should break their camouflage mechanism. To this end, we address the problem of camouflage 
								instance segmentation via the Contour Emphasis approach. We improve the ability of the segmentation models by 
								enhancing the contours of the camouflaged instances. We propose the CE-OST framework which employs the well-known 
								architecture of Transformer-based models in a one-stage manner to boost the performance of camouflaged instance 
								segmentation. The extensive experiments prove our contributions over the state-of-the-art baselines on different 
								benchmarks, i.e. CAMO++, COD10K and NC4K.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/danhntd/CE-OST" class="button medium">Code</a>
									<a style="color:rgb(0, 85, 255);" href="pdf\MAPR2023_CE-OST_Poster.pdf" class="button medium">Poster</a>
									<a style="color:rgb(0, 85, 255);" href="pdf\MAPR2023_CE-OST_Slides.pdf" class="button medium">Presentation</a>
								</p>

								<b>Reference</b>:
								<blockquote>
									<b>Thanh-Danh Nguyen</b>, Duc-Tuan Luu, Vinh-Tiep Nguyen†, and Thanh Duc Ngo, 
										“CE-OST: Contour Emphasis for One-Stage Transformer-based Camouflage Instance Segmentation.” 
										2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR). IEEE, 2023. (Scopus) 
										[<a style="color:rgb(0, 85, 255);" href="https://ieeexplore.ieee.org/document/10288682">DOI</a>]
								</blockquote>
						</article>
						
						<!-- Footer -->
						<section id="footer">
							<ul class="icons">

							</ul>
							<p class="copyright">&copy; T-Danh Nguyen</a>. Design: <a href="http://html5up.net">HTML5 UP</a>.</p>
						</section>
					</div>

				<!-- Sidebar -->
					<section id="sidebar">

						<!-- Intro -->
						<section id="intro">
							<a href="#" class="logo"><img src="images/logo.jpg" alt="" /></a>
							<header>
								<h2>Projects</h2>
							</header>
						</section>

						<!-- Intro -->
						<section id="intro">
							<h3>Contents</h3>
							<div class="mini-posts">
									
							</div>
							<article class="blurb">
								<header>
									<h4><a href="#CAMO-GenOS">CAMO-GenOS</a></h2>
									<h4><a href="#FS-CAMOFreq">FS-CAMOFreq</a></h2>
									<h4><a href="#CAMUL">CAMUL</a></h2>
									<h4><a href="#LTSP">LTSP</a></h2>
									<h4><a href="#CAMO-FS">CAMO-FS</a></h2>
									<h4><a href="#InstSynth">InstSynth</a></h2>
									<h4><a href="#CE-OST">CE-OST</a></h2>

								</header>
							Please visit my <b><a href="https://github.com/danhntd">GitHub</a></b> for more technical details.

							</article>
						</section>
						

						<section style="color:rgb(141, 141, 141);" class="blurb">
							<h2>News</h2>
							<li>Two papers about low-shot learning are accepted to <a href="https://mapr.uit.edu.vn/">MAPR</a>, Jul 2025.</li>
							<li>A journal is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=93">IEEE MultiMedia</a> (SCIE-Q1), Jun 2025.</li>
							<li>A student abstract on image segmentation with limited training data is accepted to <a href="https://aaai.org/conference/aaai/aaai-25/">AAAI</a>, 2025.</li>
							<li>Danh Nguyen was funded by the Master, PhD Scholarship Programme of <a href="https://vinif.org/">Vingroup Innovation Foundation (VinIF)</a>, code VINIF.2024.TS.068.</li>
							<li>A journal about nighttime scene understanding is accepted to <a href="https://www.sciencedirect.com/journal/image-and-vision-computing">IMAVIS</a> (SCIE-Q1), Sep 2024.</li>
							<li>A journal about few-shot learning for camouflage detection and instance segmentation is accepted to <a href="https://ieeeaccess.ieee.org/">IEEE Access</a> (SCIE-Q1), Jul 2024.</li>
							<li>A paper about conditional data synthesis for scene understanding is accepted to <a href="https://mapr.uit.edu.vn/">MAPR</a>, Jul 2024.</li>
							<li>A paper about camouflage instance segmentation is accepted to <a href="https://mapr.uit.edu.vn/">MAPR</a>, Jun 2023.</li>
							<li>Danh Nguyen has started his internship at NII, Tokyo, Japan, started in Apr 2023.</li>
							<li>A journal about cartoon face synthesis is accepted to <a href="https://link.springer.com/journal/11042">MTA</a> (SCIE-Q1), Mar 2023.</li>
							<li>Danh Nguyen was funded by the Master, PhD Scholarship Programme of <a href="https://vinif.org/">Vingroup Innovation Foundation (VinIF)</a>, code VINIF.2022.ThS.104.</li>
							<li>A paper about image stylization is accepted to <a href="http://rtdconference.info/conference/RTD-2022">RTD</a>, Nov 2022.</li>
							<li>A paper about image restoration is accepted to <a href="https://mapr.uit.edu.vn/">MAPR</a>, Oct 2022.</li>
							<li>A poster about camouflaged animals detection and segmentation is accepted to <a href="https://www.cv4animals.com/home">CV4Animals Workshop</a>, CVPR, Apr 2022.</li>
							<li>A journal about vehicle motion counting is accepted to <a href="https://link.springer.com/journal/11760">SIViP</a> (SCIE-Q2), Apr 2022.</li>
							<li>Two papers are accepted to <a href="https://mapr.uit.edu.vn/">MAPR</a>, Oct 2021.</li>
							
						</section>
						<section style="color:rgb(0, 0, 0);" class="blurb">
							<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=m&d=1uE_Kz2xVskwdjD5FXwRyJcm00jeRNnLWvqfqnfJw_A'></script>
						</section>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>